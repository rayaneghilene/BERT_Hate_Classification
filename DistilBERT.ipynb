{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayaneghilene/BERT_classification/blob/main/DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the performance DistilBERT on a classification task"
      ],
      "metadata": {
        "id": "HRv3kCbhAs1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the following variables based on your data\n",
        "* **DATAPATH**: is the path to your dataset in your environment"
      ],
      "metadata": {
        "id": "yu1eqUOsBFRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/Multitarget-CONAN (2).csv'"
      ],
      "metadata": {
        "id": "R5AN2z2jBd-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data  "
      ],
      "metadata": {
        "id": "d0xp6YLAA6S0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import the required libraries\n",
        "!pip install -q accelerate==0.21.0 --progress-bar off\n",
        "!pip install -q peft==0.4.0 --progress-bar off\n",
        "!pip install -q bitsandbytes==0.40.2 --progress-bar off\n",
        "!pip install -q transformers==4.31.0 --progress-bar off\n",
        "!pip install -q trl==0.4.7 --progress-bar off\n"
      ],
      "metadata": {
        "id": "aYSLFpR1fgqH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk --progress-bar off\n",
        "!pip install keras --progress-bar off\n",
        "!pip install tensorflow --progress-bar off\n",
        "!pip install tensorflow_hub --progress-bar off\n",
        "!pip install transformers --progress-bar off"
      ],
      "metadata": {
        "id": "sAIkFzlXfFtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import log_loss"
      ],
      "metadata": {
        "id": "ZScTyNjOA5Uq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I - DistilBERT"
      ],
      "metadata": {
        "id": "cpquq2lWBuvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data preprocessing\n",
        "df = pd.read_csv(DATAPATH)\n",
        "df[\"label\"] = df[\"TARGET\"].astype(\"category\").cat.codes  # label encoding\n",
        "data_texts = df[\"HATE_SPEECH\"].to_list()\n",
        "data_labels = df[\"label\"].to_list()\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data_texts, data_labels, test_size=0.2)\n",
        "\n",
        "# Tokenisation\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "# Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))\n",
        "\n",
        "# DistilBERT Model\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=8)  # Replace 8 with your number of classes\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.batch(16)\n",
        "test_dataset = test_dataset.batch(16)\n",
        "\n",
        "# Training\n",
        "model.fit(train_dataset, epochs=3, validation_data=test_dataset)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./distilbert-fine-tuned\")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"./distilbert-fine-tuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CboXEv2AjkfF",
        "outputId": "798455ee-0bf0-4251-9a90-fcd345563a46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "251/251 [==============================] - 90s 225ms/step - loss: 0.4271 - accuracy: 0.8986 - val_loss: 0.1432 - val_accuracy: 0.9670\n",
            "Epoch 2/3\n",
            "251/251 [==============================] - 48s 190ms/step - loss: 0.1104 - accuracy: 0.9690 - val_loss: 0.1450 - val_accuracy: 0.9630\n",
            "Epoch 3/3\n",
            "251/251 [==============================] - 47s 186ms/step - loss: 0.0803 - accuracy: 0.9758 - val_loss: 0.1231 - val_accuracy: 0.9710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at ./distilbert-fine-tuned were not used when initializing TFDistilBertForSequenceClassification: ['dropout_79']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./distilbert-fine-tuned and are newly initialized: ['dropout_99']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on a custom Prompt"
      ],
      "metadata": {
        "id": "sAqcuSaVl89c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoding = df[\"TARGET\"].astype(\"category\").cat.codes\n",
        "label_mapping = dict(enumerate(df[\"TARGET\"].astype(\"category\").cat.categories))\n",
        "\n",
        "\n",
        "custom_prompt = \"this message presents hate against muslims\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "prompt_encoding = tokenizer(custom_prompt, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "predictions = model(prompt_encoding[\"input_ids\"]).logits\n",
        "probabilities = tf.nn.softmax(predictions, axis=-1).numpy()\n",
        "predicted_class = tf.argmax(predictions, axis=-1).numpy()[0]\n",
        "predicted_label = label_mapping[predicted_class]\n",
        "\n",
        "\n",
        "print(\"Class Probabilities:\", probabilities)\n",
        "print(\"Predicted Label:\", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNfkmsDzkdo1",
        "outputId": "2e513103-74a4-4cb8-ffee-9650b3e1bc10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Probabilities: [[1.5377958e-04 2.5136684e-04 1.6974899e-04 6.6160556e-04 9.8733109e-01\n",
            "  2.6529352e-04 1.5257922e-04 1.1014453e-02]]\n",
            "Predicted Label: MUSLIMS\n"
          ]
        }
      ]
    }
  ]
}